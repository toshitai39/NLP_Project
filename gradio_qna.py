# -*- coding: utf-8 -*-
"""gradio_QnA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XoJUCqGdp1MdGXYEPGYM5uLezSQQG-0T
"""

!pip install unsloth gradio

from unsloth import FastLanguageModel
import torch
import gradio as gr
from tqdm import tqdm

def load_model():
    max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!
    dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
    load_in_4bit = True  # 4bit quantization to reduce memory usage. Can be False.

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/Llama-3.2-1B-Instruct",
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
    )

    # Prepare model for inference
    FastLanguageModel.for_inference(model)

    return model, tokenizer

# Load the model and tokenizer
model, tokenizer = load_model()

# Define the chat prompt template
chat_prompt = """
### Instruction:
{}

### Input:
{}

### Response:
{}"""

def generate_answer(question, context=""):
    # If context is provided, incorporate it into the question
    if context:
        full_question = f"Context: {context}\n\nQuestion: {question}"
    else:
        full_question = question

    # Format the prompt
    text = chat_prompt.format(
        "",  # instruction - leave blank
        full_question,  # input
        "",  # output - leave blank
    )

    # Tokenize the input
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to("cuda")

    # Generate the response
    outputs = model.generate(**inputs, max_new_tokens=250, num_return_sequences=1)

    # Decode the output
    text_out = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extract only the response part (after "### Response:")
    response_parts = text_out.split("### Response:")
    if len(response_parts) > 1:
        return response_parts[1].strip()
    else:
        return text_out

# Create the Gradio interface
with gr.Blocks(title="Context-Aware Question Answering") as demo:
    gr.Markdown("# Context-Aware Question Answering")
    gr.Markdown("Ask a question and optionally provide context to get an answer from the fine-tuned Llama model.")

    with gr.Row():
        with gr.Column():
            context_input = gr.Textbox(
                label="Context (Optional)",
                placeholder="Enter relevant context information here...",
                lines=5
            )
            question_input = gr.Textbox(
                label="Question",
                placeholder="Enter your question here..."
            )
            submit_btn = gr.Button("Generate Answer")

        with gr.Column():
            answer_output = gr.Textbox(
                label="Generated Answer",
                lines=10
            )

    submit_btn.click(
        fn=generate_answer,
        inputs=[question_input, context_input],
        outputs=answer_output
    )

    gr.Examples(
        [
            ["What is the capital of France?", ""],
            ["Explain the process of photosynthesis.", ""],
            ["Who won the 2023 Cricket World Cup?", "India and Australia played in the final match."]
        ],
        inputs=[question_input, context_input]
    )

# Launch the app
demo.launch()